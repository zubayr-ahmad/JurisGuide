{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This implementation is using LangGraph to implement the chatting system\n",
    "import sqlite3\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import TypedDict, List, Annotated, Dict, Any\n",
    "from langgraph.graph import StateGraph, END\n",
    "import uuid\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import json\n",
    "import time\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Database and Vector Store paths\n",
    "DB_PATH = os.getenv(\"DB_PATH\", \"data/chat_history.db\")\n",
    "VECTOR_STORE_PATH = os.getenv(\"VECTOR_STORE_PATH\", \"data/chroma_db\")\n",
    "PROCESSED_FOLDER = \"data/processed\"\n",
    "LLM_API_KEY = os.getenv(\"LLM_API_KEY\")\n",
    "LLM_MODEL = os.getenv(\"LLM_MODEL\")\n",
    "HISTORY_CONTEXT = 5\n",
    "RETRIEVE_DOCS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database Layer\n",
    "class ChatDatabase:\n",
    "    def __init__(self):\n",
    "        self.conn = sqlite3.connect(DB_PATH, check_same_thread=False)\n",
    "        self._create_table()\n",
    "\n",
    "    def _create_table(self):\n",
    "        # print(\"Creating tables >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
    "        self.conn.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS chats (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                session_id TEXT,\n",
    "                user_message TEXT,\n",
    "                response TEXT,\n",
    "                reference_docs TEXT,\n",
    "                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP\n",
    "            )\n",
    "        \"\"\")\n",
    "        self.conn.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS sessions (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                session_id TEXT,\n",
    "                name TEXT,\n",
    "                created_at DATETIME DEFAULT CURRENT_TIMESTAMP\n",
    "            )\n",
    "        \"\"\")\n",
    "        self.conn.commit()\n",
    "\n",
    "    def save_chat(self, session_id: str, user_message: str, response: str, reference_docs=None):\n",
    "        \"\"\"Save a chat message with references stored as JSON\"\"\"\n",
    "        cursor = self.conn.cursor()\n",
    "        \n",
    "        # Convert list of Documents to list of dictionaries\n",
    "        references = json.dumps([\n",
    "            {\"page_content\": doc.page_content, \"metadata\": doc.metadata}\n",
    "            for doc in (reference_docs or [])\n",
    "        ])\n",
    "        \n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT INTO chats (session_id, user_message, response, reference_docs)\n",
    "            VALUES (?, ?, ?, ?)\n",
    "        \"\"\", (session_id, user_message, response, references))\n",
    "        \n",
    "        self.conn.commit()\n",
    "    def create_session(self, session_id: str, metadata: dict):\n",
    "        \"\"\"Create a new session with metadata\"\"\"\n",
    "        # Add this method to save session metadata (name, created_at) to your database\n",
    "        # Implementation depends on your database structure\n",
    "        cursor = self.conn.cursor()\n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT INTO sessions (session_id, name, created_at)\n",
    "            VALUES (?, ?, ?)\n",
    "        \"\"\", (session_id, metadata['name'], metadata['created_at']))\n",
    "        self.conn.commit()\n",
    "\n",
    "    def get_all_sessions(self):\n",
    "        \"\"\"Retrieve all sessions with their metadata\"\"\"\n",
    "        cursor = self.conn.cursor()\n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT session_id, name, created_at\n",
    "            FROM sessions\n",
    "        \"\"\")\n",
    "        sessions = {}\n",
    "        for row in cursor.fetchall():\n",
    "            sessions[row[0]] = {\n",
    "                'name': row[1],\n",
    "                'created_at': row[2]\n",
    "            }\n",
    "        return sessions\n",
    "\n",
    "    def get_chat_history(self, session_id: str, limit=HISTORY_CONTEXT):\n",
    "        \"\"\"Retrieve chat history for a session with reference docs as list of dicts\"\"\"\n",
    "        cursor = self.conn.cursor()\n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT user_message, response, reference_docs\n",
    "            FROM chats\n",
    "            WHERE session_id = ?\n",
    "            ORDER BY timestamp DESC LIMIT ?\n",
    "        \"\"\", (session_id, limit))\n",
    "        \n",
    "        history = []\n",
    "        for row in cursor.fetchall():\n",
    "            user_message, response, reference_docs_json = row\n",
    "            reference_docs = json.loads(reference_docs_json) if reference_docs_json else []\n",
    "            \n",
    "            history.append({\n",
    "                'user_message': user_message,\n",
    "                'response': response,\n",
    "                'reference_docs': reference_docs\n",
    "            })\n",
    "        \n",
    "        return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Retrieval Layer\n",
    "class DocumentRetriever:\n",
    "    def __init__(self):\n",
    "        self.embeddings = HuggingFaceEmbeddings()\n",
    "        self.processed_files_path = os.path.join(VECTOR_STORE_PATH, \"processed_files.json\")\n",
    "        self.processed_files = self._load_processed_files()\n",
    "        self.db = self._initialize_vectorstore()\n",
    "\n",
    "    def _load_processed_files(self) -> Dict[str, Dict]:\n",
    "        \"\"\"Load the list of processed files and their metadata.\"\"\"\n",
    "        if os.path.exists(self.processed_files_path):\n",
    "            with open(self.processed_files_path, 'r') as f:\n",
    "                return json.load(f)\n",
    "        return {}\n",
    "\n",
    "    def _save_processed_files(self):\n",
    "        \"\"\"Save the list of processed files and their metadata.\"\"\"\n",
    "        os.makedirs(VECTOR_STORE_PATH, exist_ok=True)\n",
    "        with open(self.processed_files_path, 'w') as f:\n",
    "            json.dump(self.processed_files, f, indent=2)\n",
    "\n",
    "    def _get_file_metadata(self, filepath: str) -> Dict:\n",
    "        \"\"\"Get file metadata including modification time and size.\"\"\"\n",
    "        stat = os.stat(filepath)\n",
    "        return {\n",
    "            \"mtime\": stat.st_mtime,\n",
    "            \"size\": stat.st_size\n",
    "        }\n",
    "\n",
    "    def _has_file_changed(self, filepath: str) -> bool:\n",
    "        \"\"\"Check if a file has been modified since last processing.\"\"\"\n",
    "        if not os.path.exists(filepath):\n",
    "            return False\n",
    "\n",
    "        current_metadata = self._get_file_metadata(filepath)\n",
    "        filename = os.path.basename(filepath)\n",
    "\n",
    "        if filename not in self.processed_files:\n",
    "            return True\n",
    "\n",
    "        stored_metadata = self.processed_files[filename]\n",
    "        return (current_metadata[\"mtime\"] != stored_metadata[\"mtime\"] or\n",
    "                current_metadata[\"size\"] != stored_metadata[\"size\"])\n",
    "\n",
    "    def _initialize_vectorstore(self):\n",
    "        \"\"\"Initialize or update the vector store with new or modified documents.\"\"\"\n",
    "        if not os.path.exists(PROCESSED_FOLDER):\n",
    "            os.makedirs(PROCESSED_FOLDER)\n",
    "\n",
    "        # Get all PDF files in the processed folder\n",
    "        pdf_files = [f for f in os.listdir(PROCESSED_FOLDER) if f.endswith(\".pdf\")]\n",
    "        \n",
    "        # Check for new or modified files\n",
    "        new_docs = []\n",
    "        for filename in pdf_files:\n",
    "            filepath = os.path.join(PROCESSED_FOLDER, filename)\n",
    "            \n",
    "            if self._has_file_changed(filepath):\n",
    "                print(f\"Processing new/modified file: {filename}\")\n",
    "                loader = PyPDFLoader(filepath)\n",
    "                new_docs.extend(loader.load())\n",
    "                \n",
    "                # Update processed files record\n",
    "                self.processed_files[filename] = self._get_file_metadata(filepath)\n",
    "        \n",
    "        # Save the updated processed files record\n",
    "        self._save_processed_files()\n",
    "\n",
    "        # If we have new documents, process them and update the vector store\n",
    "        if new_docs:\n",
    "            print(f\"Number of new documents to process: {len(new_docs)}\")\n",
    "            text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=300)\n",
    "            texts = text_splitter.split_documents(new_docs)\n",
    "            print(f\"Created {len(texts)} chunks from new documents\")\n",
    "            \n",
    "            # If vector store exists, add to it; otherwise create new\n",
    "            if os.path.exists(VECTOR_STORE_PATH):\n",
    "                db = Chroma(persist_directory=VECTOR_STORE_PATH, \n",
    "                          embedding_function=self.embeddings)\n",
    "                db.add_documents(texts)\n",
    "                return db\n",
    "            else:\n",
    "                return Chroma.from_documents(texts, self.embeddings, \n",
    "                                          persist_directory=VECTOR_STORE_PATH)\n",
    "        \n",
    "        # If no new documents, just load existing vector store\n",
    "        return Chroma(persist_directory=VECTOR_STORE_PATH, \n",
    "                     embedding_function=self.embeddings)\n",
    "\n",
    "    def retrieve_documents(self, query: str, k=RETRIEVE_DOCS) -> List[Any]:\n",
    "        \"\"\"Retrieve similar documents for a given query.\"\"\"\n",
    "        return self.db.similarity_search(query, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Response Generation\n",
    "class ResponseGenerator:\n",
    "    def __init__(self):\n",
    "        self.llm = ChatGroq(api_key=LLM_API_KEY, \n",
    "                          model=LLM_MODEL)\n",
    "\n",
    "    def generate_response(self, context: str, history: str, query: str) -> str:\n",
    "        messages = [\n",
    "            SystemMessage(content=\"You are an AI assistant. Use the provided context to generate accurate answers. If the context is insufficient, state that you are unsure rather than guessing.\"),\n",
    "            HumanMessage(content=f\"Context:\\n{context}\\n\\nConversation History:\\n{history}\\n\\nUser Query:\\n{query}\\n\\nResponse:\")\n",
    "        ]\n",
    "\n",
    "        result = self.llm.stream(messages)\n",
    "        print(f\"Generated response: {result}\")\n",
    "        return result\n",
    "    \n",
    "    def custom_call(self, user_prompt, system_prompt=None):\n",
    "        messages = [HumanMessage(content=user_prompt)]\n",
    "        if system_prompt:\n",
    "            messages.insert(0, SystemMessage(system_prompt))\n",
    "        return self.llm(messages).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangGraph State Definition\n",
    "class ChatState(TypedDict):\n",
    "    session_id: str\n",
    "    user_message: str\n",
    "    reference_docs: List[Any]\n",
    "    response: str\n",
    "    requires_retrieval: bool\n",
    "    history: Annotated[List[Dict[str, str]], lambda x, y: x + y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangGraph Nodes\n",
    "class ChatNodes:\n",
    "    def __init__(self):\n",
    "        self.db = ChatDatabase()\n",
    "        self.retriever = DocumentRetriever()\n",
    "        self.generator = ResponseGenerator()\n",
    "\n",
    "    def retrieve_documents(self, state: ChatState) -> Dict:\n",
    "        docs = self.retriever.retrieve_documents(state[\"user_message\"])\n",
    "        return {\"reference_docs\": docs}\n",
    "\n",
    "    def generate_response(self, state: ChatState) -> Dict:\n",
    "        context = \"\\n\\n\".join([doc.page_content for doc in state[\"reference_docs\"]])\n",
    "        history = \"\\n\".join([f\"User: {msg['user_message']}\\nBot: {msg['response']}\" \n",
    "                           for msg in state.get(\"history\", [])[-HISTORY_CONTEXT:]])\n",
    "        # print(\"Context >>>>>>>>>>>>>>>>>>>>>>>>\", context)\n",
    "        # print(\"History >>>>>>>>>>>>>>>>>>>>>>>>\", history)\n",
    "        # response = self.generator.generate_response(\n",
    "        #     context=context,\n",
    "        #     history=history,\n",
    "        #     query=state[\"user_message\"]\n",
    "        # )\n",
    "        generator = self.generator.generate_response(\n",
    "            context=context,\n",
    "            history=history,\n",
    "            query=state[\"user_message\"]\n",
    "        )\n",
    "        # print(\"Response from generator >>>>>>>>>>>>>>>>>>>>>>>>\", generator)\n",
    "        return generator\n",
    "        \n",
    "\n",
    "    def save_conversation(self, state: ChatState) -> Dict:\n",
    "        self.db.save_chat(\n",
    "            state[\"session_id\"],\n",
    "            state[\"user_message\"],\n",
    "            state[\"response\"],\n",
    "            reference_docs=state[\"reference_docs\"]\n",
    "        )\n",
    "        return {\n",
    "            \"history\": [{\n",
    "                \"user_message\": state[\"user_message\"],\n",
    "                \"response\": state[\"response\"],\n",
    "                \"reference_docs\": state[\"reference_docs\"]\n",
    "            }]\n",
    "        }\n",
    "    \n",
    "    def decide_retrieval(self, state: ChatState):\n",
    "        \"\"\"Uses the LLM to decide whether retrieval is needed.\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        You are an AI assistant that determines whether a user's query requires retrieving external knowledge.\n",
    "\n",
    "        If the query is a greeting or a general conversational question (like \"How are you?\"), respond `False`.\n",
    "        If the query asks about specific knowledge (like documentation, facts, or other stored data), respond `True`.\n",
    "\n",
    "        Query: \"{state['user_message']}\"\n",
    "        Answer with 'True' if retrieval is needed, otherwise 'False'. It should be either of them in all cases\n",
    "        \"\"\"\n",
    "\n",
    "        response = self.generator.custom_call(prompt)  \n",
    "        # print(\"Response from decider >>>>>>>>>>>>>>>>>>>>>>>>\", response)\n",
    "        state['requires_retrieval'] = \"true\" in response.lower().strip() \n",
    "        print(\"State\", state)\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangGraph Workflow Setup with LLM Classification\n",
    "def create_workflow():\n",
    "    nodes = ChatNodes()\n",
    "    workflow = StateGraph(ChatState)\n",
    "\n",
    "    workflow.add_node(\"decide_retrieval\", nodes.decide_retrieval)  # LLM decides if retrieval is needed\n",
    "    workflow.add_node(\"retrieve\", nodes.retrieve_documents)\n",
    "    workflow.add_node(\"generate\", nodes.generate_response)\n",
    "    workflow.add_node(\"save\", nodes.save_conversation)\n",
    "\n",
    "    workflow.set_entry_point(\"decide_retrieval\")\n",
    "\n",
    "    # Conditional transition based on LLM decision\n",
    "    workflow.add_conditional_edges(\n",
    "        \"decide_retrieval\",\n",
    "        lambda state: \"retrieve\" if state['requires_retrieval'] else \"generate\",\n",
    "        {\"retrieve\": \"retrieve\", \"generate\": \"generate\"},\n",
    "    )\n",
    "\n",
    "    workflow.add_edge(\"retrieve\", \"generate\")\n",
    "    workflow.add_edge(\"generate\", \"save\")\n",
    "    workflow.add_edge(\"save\", END)\n",
    "\n",
    "    return workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LangGraphChat:\n",
    "    def __init__(self):\n",
    "        self.workflow = create_workflow()\n",
    "        self.db = ChatDatabase()\n",
    "\n",
    "    def chat(self, user_message: str, session_id: str = None):\n",
    "        session_id = session_id or str(uuid.uuid4())\n",
    "        history = self.db.get_chat_history(session_id, limit=HISTORY_CONTEXT)\n",
    "        initial_state = {\n",
    "            \"session_id\": session_id,\n",
    "            \"user_message\": user_message,\n",
    "            \"reference_docs\": [],\n",
    "            \"response\": \"\",\n",
    "            \"history\": [\n",
    "                {\"user_message\": h['user_message'], \"response\": h[\"response\"], \"reference_docs\": h['reference_docs']}\n",
    "                for h in history\n",
    "            ]\n",
    "        }\n",
    "        return self.workflow.stream(initial_state)\n",
    "\n",
    "        # full_response = \"\"\n",
    "        # print(\"Generator \", self.workflow.stream(initial_state))\n",
    "        # print(\"Type\", type(self.workflow.stream(initial_state)))\n",
    "        # This is your generator streaming updates\n",
    "        # for update in self.workflow.stream(initial_state):\n",
    "        #     # Check if a response chunk is available\n",
    "        #     if \"generate\" in update:\n",
    "        #         print(\"Update >>>>>>>>>>>>>>>>>>>>>>>>\", update)\n",
    "        #         full_response += update[\"generate\"]\n",
    "        #         print(\"Full Response >>>>>>>>>>>>>>>>>>>>>>>>\", full_response)\n",
    "        #         yield update[\"generate\"]\n",
    "        #     # You can also yield final payloads or metadata as needed.\n",
    "        # # Optionally yield a final payload with metadata at the end.\n",
    "        # yield {\"final\": True, \"session_id\": session_id, \"response\": full_response, \"reference_docs\": initial_state[\"reference_docs\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot = LangGraphChat()\n",
    "result = chatbot.chat(\"Hello\")\n",
    "# print(f\"Response: {result['response']}\")\n",
    "# print(f\"Session ID: {result['session_id']}\")\n",
    "# print(f\"References: {result['reference_docs']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Pregel.stream at 0x000001BBB67FFC50>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = next(result)\n",
    "# print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b = next(result)\n",
    "# print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State {'session_id': '8cb412c0-0a88-4bf8-a46d-ec51c805283a', 'user_message': 'Hello', 'reference_docs': [], 'response': '', 'history': [], 'requires_retrieval': False}\n",
      "{'decide_retrieval': {'session_id': '8cb412c0-0a88-4bf8-a46d-ec51c805283a', 'user_message': 'Hello', 'reference_docs': [], 'response': '', 'history': [], 'requires_retrieval': False}}\n",
      "Generated response: <generator object BaseChatModel.stream at 0x000001BB80BC6710>\n"
     ]
    },
    {
     "ename": "InvalidUpdateError",
     "evalue": "Expected dict, got <generator object BaseChatModel.stream at 0x000001BB80BC6710>\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_GRAPH_NODE_RETURN_VALUE",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidUpdateError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[109], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zubay\\.conda\\envs\\ml_env\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:1670\u001b[0m, in \u001b[0;36mPregel.stream\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[0;32m   1664\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[0;32m   1665\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates.\u001b[39;00m\n\u001b[0;32m   1666\u001b[0m     \u001b[38;5;66;03m# Channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[0;32m   1667\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[0;32m   1668\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps.\u001b[39;00m\n\u001b[0;32m   1669\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels):\n\u001b[1;32m-> 1670\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1671\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1672\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1673\u001b[0m \u001b[43m            \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1674\u001b[0m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1675\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1676\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[0;32m   1677\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1678\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\zubay\\.conda\\envs\\ml_env\\Lib\\site-packages\\langgraph\\pregel\\runner.py:230\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[1;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[0;32m    228\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 230\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_SEND\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Users\\zubay\\.conda\\envs\\ml_env\\Lib\\site-packages\\langgraph\\pregel\\retry.py:40\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[1;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[0;32m     38\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     42\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[1;32mc:\\Users\\zubay\\.conda\\envs\\ml_env\\Lib\\site-packages\\langgraph\\utils\\runnable.py:464\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    462\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    463\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 464\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\zubay\\.conda\\envs\\ml_env\\Lib\\site-packages\\langgraph\\utils\\runnable.py:218\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    216\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[0;32m    217\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[1;32m--> 218\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    220\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32mc:\\Users\\zubay\\.conda\\envs\\ml_env\\Lib\\site-packages\\langgraph\\pregel\\write.py:96\u001b[0m, in \u001b[0;36mChannelWrite._write\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_write\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Any, config: RunnableConfig) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     88\u001b[0m     writes \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     89\u001b[0m         ChannelWriteEntry(write\u001b[38;5;241m.\u001b[39mchannel, \u001b[38;5;28minput\u001b[39m, write\u001b[38;5;241m.\u001b[39mskip_none, write\u001b[38;5;241m.\u001b[39mmapper)\n\u001b[0;32m     90\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(write, ChannelWriteEntry) \u001b[38;5;129;01mand\u001b[39;00m write\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;129;01mis\u001b[39;00m PASSTHROUGH\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m write \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrites\n\u001b[0;32m     95\u001b[0m     ]\n\u001b[1;32m---> 96\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_write\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_at_least_one_of\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\zubay\\.conda\\envs\\ml_env\\Lib\\site-packages\\langgraph\\pregel\\write.py:143\u001b[0m, in \u001b[0;36mChannelWrite.do_write\u001b[1;34m(config, writes, require_at_least_one_of)\u001b[0m\n\u001b[0;32m    141\u001b[0m     tuples\u001b[38;5;241m.\u001b[39mappend((TASKS, w))\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(w, ChannelWriteTupleEntry):\n\u001b[1;32m--> 143\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ww \u001b[38;5;241m:=\u001b[39m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    144\u001b[0m         tuples\u001b[38;5;241m.\u001b[39mextend(ww)\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(w, ChannelWriteEntry):\n",
      "File \u001b[1;32mc:\\Users\\zubay\\.conda\\envs\\ml_env\\Lib\\site-packages\\langgraph\\graph\\state.py:692\u001b[0m, in \u001b[0;36mCompiledStateGraph.attach_node.<locals>._get_updates\u001b[1;34m(input)\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    688\u001b[0m     msg \u001b[38;5;241m=\u001b[39m create_error_message(\n\u001b[0;32m    689\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected dict, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    690\u001b[0m         error_code\u001b[38;5;241m=\u001b[39mErrorCode\u001b[38;5;241m.\u001b[39mINVALID_GRAPH_NODE_RETURN_VALUE,\n\u001b[0;32m    691\u001b[0m     )\n\u001b[1;32m--> 692\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidUpdateError(msg)\n",
      "\u001b[1;31mInvalidUpdateError\u001b[0m: Expected dict, got <generator object BaseChatModel.stream at 0x000001BB80BC6710>\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_GRAPH_NODE_RETURN_VALUE",
      "\u001b[0mDuring task with name 'generate' and id '7c380370-55c5-1b40-f26b-398a069a9e4d'"
     ]
    }
   ],
   "source": [
    "for i in result:\n",
    "    time.sleep(2)\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Response Generation\n",
    "class ResponseGenerator:\n",
    "    def __init__(self):\n",
    "        self.llm = ChatGroq(api_key=LLM_API_KEY, \n",
    "                          model=LLM_MODEL)\n",
    "\n",
    "    def generate_response(self, context: str, history: str, query: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Generate a response and return a dictionary with a placeholder.\n",
    "        The actual streaming happens outside the LangGraph workflow.\n",
    "        \"\"\"\n",
    "        # Return a placeholder to satisfy LangGraph's requirement for a dict\n",
    "        return {\"response\": \"Response will be streamed directly in the UI\"}\n",
    "    \n",
    "    def get_stream(self, context: str, history: str, query: str):\n",
    "        \"\"\"Method that returns the stream generator directly\"\"\"\n",
    "        messages = [\n",
    "            SystemMessage(content=\"You are an AI assistant. Use the provided context to generate accurate answers. If the context is insufficient, state that you are unsure rather than guessing.\"),\n",
    "            HumanMessage(content=f\"Context:\\n{context}\\n\\nConversation History:\\n{history}\\n\\nUser Query:\\n{query}\\n\\nResponse:\")\n",
    "        ]\n",
    "\n",
    "        return self.llm.stream(messages)\n",
    "    \n",
    "    def custom_call(self, user_prompt, system_prompt=None):\n",
    "        messages = [HumanMessage(content=user_prompt)]\n",
    "        if system_prompt:\n",
    "            messages.insert(0, SystemMessage(system_prompt))\n",
    "        return self.llm(messages).content\n",
    "\n",
    "# LangGraph State Definition\n",
    "class ChatState(TypedDict):\n",
    "    session_id: str\n",
    "    user_message: str\n",
    "    reference_docs: List[Any]\n",
    "    response: str\n",
    "    requires_retrieval: bool\n",
    "    context: str  # Added to store context for streaming outside the graph\n",
    "    chat_history: str  # Added to store history for streaming outside the graph\n",
    "    history: Annotated[List[Dict[str, str]], lambda x, y: x + y]\n",
    "\n",
    "# LangGraph Nodes\n",
    "class ChatNodes:\n",
    "    def __init__(self):\n",
    "        self.db = ChatDatabase()\n",
    "        self.retriever = DocumentRetriever()\n",
    "        self.generator = ResponseGenerator()\n",
    "\n",
    "    def retrieve_documents(self, state: ChatState) -> Dict:\n",
    "        docs = self.retriever.retrieve_documents(state[\"user_message\"])\n",
    "        # Also prepare the context string for later streaming\n",
    "        context = \"\\n\\n\".join([doc.page_content for doc in state[\"reference_docs\"] + docs])\n",
    "        return {\"reference_docs\": docs, \"context\": context}\n",
    "\n",
    "    def generate_response(self, state: ChatState) -> Dict:\n",
    "        # If no documents were retrieved, create the context here\n",
    "        if not state.get(\"context\"):\n",
    "            context = \"\\n\\n\".join([doc.page_content for doc in state[\"reference_docs\"]])\n",
    "        else:\n",
    "            context = state[\"context\"]\n",
    "            \n",
    "        history = \"\\n\".join([f\"User: {msg['user_message']}\\nBot: {msg['response']}\" \n",
    "                           for msg in state.get(\"history\", [])[-HISTORY_CONTEXT:]])\n",
    "        \n",
    "        # Store context and history for later streaming\n",
    "        return {\n",
    "            \"context\": context,\n",
    "            \"chat_history\": history,\n",
    "            \"response\": \"Response will be streamed directly in the UI\"\n",
    "        }\n",
    "        \n",
    "    def save_conversation(self, state: ChatState) -> Dict:\n",
    "        # For now, the response is just a placeholder\n",
    "        # The actual response will be streamed directly in the UI\n",
    "        # We'll update the database after streaming is complete\n",
    "        return {\n",
    "            \"history\": [{\n",
    "                \"user_message\": state[\"user_message\"],\n",
    "                \"response\": state[\"response\"],\n",
    "                \"reference_docs\": state[\"reference_docs\"]\n",
    "            }]\n",
    "        }\n",
    "    \n",
    "    def decide_retrieval(self, state: ChatState):\n",
    "        \"\"\"Uses the LLM to decide whether retrieval is needed.\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        You are an AI assistant that determines whether a user's query requires retrieving external knowledge.\n",
    "\n",
    "        If the query is a greeting or a general conversational question (like \"How are you?\"), respond `False`.\n",
    "        If the query asks about specific knowledge (like documentation, facts, or other stored data), respond `True`.\n",
    "\n",
    "        Query: \"{state['user_message']}\"\n",
    "        Answer with 'True' if retrieval is needed, otherwise 'False'. It should be either of them in all cases\n",
    "        \"\"\"\n",
    "\n",
    "        response = self.generator.custom_call(prompt)  \n",
    "        state['requires_retrieval'] = \"true\" in response.lower().strip() \n",
    "        print(\"State\", state)\n",
    "        return state\n",
    "\n",
    "# LangGraph Workflow Setup with LLM Classification\n",
    "def create_workflow():\n",
    "    nodes = ChatNodes()\n",
    "    workflow = StateGraph(ChatState)\n",
    "\n",
    "    workflow.add_node(\"decide_retrieval\", nodes.decide_retrieval)\n",
    "    workflow.add_node(\"retrieve\", nodes.retrieve_documents)\n",
    "    workflow.add_node(\"generate\", nodes.generate_response)\n",
    "    workflow.add_node(\"save\", nodes.save_conversation)\n",
    "\n",
    "    workflow.set_entry_point(\"decide_retrieval\")\n",
    "\n",
    "    workflow.add_conditional_edges(\n",
    "        \"decide_retrieval\",\n",
    "        lambda state: \"retrieve\" if state['requires_retrieval'] else \"generate\",\n",
    "        {\"retrieve\": \"retrieve\", \"generate\": \"generate\"},\n",
    "    )\n",
    "\n",
    "    workflow.add_edge(\"retrieve\", \"generate\")\n",
    "    workflow.add_edge(\"generate\", \"save\")\n",
    "    workflow.add_edge(\"save\", END)\n",
    "\n",
    "    return workflow.compile()\n",
    "\n",
    "class LangGraphChat:\n",
    "    def __init__(self):\n",
    "        self.workflow = create_workflow()\n",
    "        self.db = ChatDatabase()\n",
    "        self.generator = ResponseGenerator()\n",
    "\n",
    "    def chat(self, user_message: str, session_id: str = None):\n",
    "        \"\"\"\n",
    "        Process the chat through LangGraph and then return a stream generator \n",
    "        that can be used in Streamlit.\n",
    "        \"\"\"\n",
    "        session_id = session_id or str(uuid.uuid4())\n",
    "        history = self.db.get_chat_history(session_id, limit=HISTORY_CONTEXT)\n",
    "        initial_state = {\n",
    "            \"session_id\": session_id,\n",
    "            \"user_message\": user_message,\n",
    "            \"reference_docs\": [],\n",
    "            \"response\": \"\",\n",
    "            \"context\": \"\",\n",
    "            \"chat_history\": \"\",\n",
    "            \"history\": [\n",
    "                {\"user_message\": h['user_message'], \"response\": h[\"response\"], \"reference_docs\": h['reference_docs']}\n",
    "                for h in history\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Run the workflow to prepare everything\n",
    "        final_state = self.workflow.invoke(initial_state)\n",
    "        \n",
    "        # Now get the stream directly from the LLM\n",
    "        stream = self.generator.get_stream(\n",
    "            context=final_state[\"context\"],\n",
    "            history=final_state[\"chat_history\"],\n",
    "            query=user_message\n",
    "        )\n",
    "        \n",
    "        # Return both the session ID and the stream for use in Streamlit\n",
    "        return session_id, stream\n",
    "    \n",
    "    def save_streamed_response(self, session_id, user_message, response, reference_docs):\n",
    "        \"\"\"\n",
    "        Save the final streamed response to the database\n",
    "        \"\"\"\n",
    "        self.db.save_chat(\n",
    "            session_id,\n",
    "            user_message,\n",
    "            response,\n",
    "            reference_docs=reference_docs\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State {'session_id': 'fab50fd9-947e-45a6-9bf5-688fcd9c789f', 'user_message': 'Hello, can you help me understand quantum computing?', 'reference_docs': [], 'response': '', 'context': '', 'chat_history': '', 'history': [], 'requires_retrieval': True}\n",
      "Session ID: fab50fd9-947e-45a6-9bf5-688fcd9c789f\n",
      "Streaming response:\n",
      "I'm unsure about the provided context relating to quantum computing. The context appears to be an Explanatory Guide on Federal Decree-Law No. 47 of 2022 on the Taxation of Corporations and Businesses, specifically discussing articles related to corporate tax laws, tax losses, and assessment of corporate tax. \n",
      "\n",
      "If you'd like to ask a question about quantum computing or discuss a different topic, I'll be happy to assist you.\n",
      "\n",
      "Full response: I'm unsure about the provided context relating to quantum computing. The context appears to be an Explanatory Guide on Federal Decree-Law No. 47 of 2022 on the Taxation of Corporations and Businesses, specifically discussing articles related to corporate tax laws, tax losses, and assessment of corporate tax. \n",
      "\n",
      "If you'd like to ask a question about quantum computing or discuss a different topic, I'll be happy to assist you.\n"
     ]
    }
   ],
   "source": [
    "# For testing\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot = LangGraphChat()\n",
    "    \n",
    "    session_id, stream = chatbot.chat(\"Hello, can you help me understand quantum computing?\")\n",
    "    \n",
    "    print(f\"Session ID: {session_id}\")\n",
    "    print(\"Streaming response:\")\n",
    "    \n",
    "    full_response = \"\"\n",
    "    for chunk in stream:\n",
    "        if hasattr(chunk, 'content'):\n",
    "            content = chunk.content\n",
    "            time.sleep(2)\n",
    "        else:\n",
    "            content = str(chunk)\n",
    "            \n",
    "        if content:\n",
    "            full_response += content\n",
    "            print(content, end='', flush=True)\n",
    "    \n",
    "    print(\"\\n\\nFull response:\", full_response)\n",
    "    \n",
    "    # Now save the full response\n",
    "    chatbot.save_streamed_response(\n",
    "        session_id,\n",
    "        \"Hello, can you help me understand quantum computing?\",\n",
    "        full_response,\n",
    "        []  # Example reference docs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a3949cea-bfaf-4150-9acb-ffc2b1c522d0\n",
    "response = chatbot.chat(\"Can you elaborate on point 3?\", session_id=\"a3949cea-bfaf-4150-9acb-ffc2b1c522d0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Pregel.stream at 0x00000212021B0610>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from decider >>>>>>>>>>>>>>>>>>>>>>>> False\n",
      "State {'session_id': 'a3949cea-bfaf-4150-9acb-ffc2b1c522d0', 'user_message': 'Can you elaborate on point 3?', 'reference_docs': [], 'response': '', 'history': [], 'requires_retrieval': False}\n",
      "{'decide_retrieval': {'session_id': 'a3949cea-bfaf-4150-9acb-ffc2b1c522d0', 'user_message': 'Can you elaborate on point 3?', 'reference_docs': [], 'response': '', 'history': [], 'requires_retrieval': False}}\n",
      "Context >>>>>>>>>>>>>>>>>>>>>>>> \n",
      "History >>>>>>>>>>>>>>>>>>>>>>>> \n",
      "{'generate': {'response': \"I'm happy to help, but there is no conversation history or user query provided for me to base my response on. Please provide the necessary context or information so I can assist you accurately.\"}}\n",
      "{'save': {'history': [{'user_message': 'Can you elaborate on point 3?', 'response': \"I'm happy to help, but there is no conversation history or user query provided for me to base my response on. Please provide the necessary context or information so I can assist you accurately.\", 'reference_docs': []}]}}\n"
     ]
    }
   ],
   "source": [
    "for i in response:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'generator' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'generator' object has no attribute 'keys'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = chatbot.db.get_chat_history(\"a3949cea-bfaf-4150-9acb-ffc2b1c522d0\", limit=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('What is the refund policy?',\n",
       "  'Based on the provided context, the refund policy involves the following steps:\\n\\n1. The Bureau Expo 2020 Dubai makes a request to the Authority to refund the amount, provided the refund claim is correct.\\n2. The Authority and Bureau Expo 2020 Dubai agree on procedural, evidential, and verification requirements to be met by the Office of the Official Participant or any other Person to be eligible for the refund claim (Article 4: Requirements for Refund).\\n3. Where the refund claim is correct, the Bureau Expo 2020 Dubai makes a request to the Authority to refund the amount, and the Authority prepares a Certificate on Entitlement (Article 5: Certificate on Entitlement).\\n\\nTherefore, the refund policy is that the Bureau Expo 2020 Dubai requests the Authority for a refund if the claim is correct, after meeting the agreed-upon requirements and obtaining the Certificate on Entitlement from the Authority.'),\n",
       " ('Can you elaborate on point 3?',\n",
       "  \"Based on the provided context, point 3 states that where the refund claim is correct, the Bureau Expo 2020 Dubai makes a request to the Authority to refund the amount, and the Authority prepares a Certificate on Entitlement.\\n\\nTo elaborate, the Certificate on Entitlement is a document issued by the Authority in response to the request made by the Bureau Expo 2020 Dubai. The Certificate serves as evidence of the entitlement to the refund, and it is typically prepared by the Authority after verifying the refund claim and meeting the agreed-upon requirements.\\n\\nThe Certificate on Entitlement would likely include information such as the name of the Official Participant, the amount of the refund, and any other relevant details. The Bureau Expo 2020 Dubai would then use this Certificate to support their request for a refund to the Authority.\\n\\nIt's worth noting that the specific requirements for the Certificate on Entitlement are not explicitly stated in the provided context, but it is likely that the Authority would require the Bureau Expo 2020 Dubai to provide documentation or evidence to support the refund claim, and the Certificate would be a formal acknowledgment of the entitlement to the refund.\"),\n",
       " ('Can you elaborate on point 3?',\n",
       "  \"Based on the provided context, point 3 states that where the refund claim is correct, the Bureau Expo 2020 Dubai makes a request to the Authority to refund the amount, and the Authority prepares a Certificate on Entitlement.\\n\\nTo elaborate, the Certificate on Entitlement is a document issued by the Authority in response to the request made by the Bureau Expo 2020 Dubai. The Certificate serves as evidence of the entitlement to the refund, and it is typically prepared by the Authority after verifying the refund claim and meeting the agreed-upon requirements.\\n\\nThe Certificate on Entitlement would likely include information such as:\\n\\n- The name of the Official Participant\\n- The amount of the refund\\n- Any other relevant details\\n\\nThe Bureau Expo 2020 Dubai would then use this Certificate to support their request for a refund to the Authority. It's worth noting that the specific requirements for the Certificate on Entitlement are not explicitly stated in the provided context, but it is likely that the Authority would require the Bureau Expo 2020 Dubai to provide documentation or evidence to support the refund claim, and the Certificate would be a formal acknowledgment of the entitlement to the refund.\"),\n",
       " ('Can you elaborate on point 3?',\n",
       "  \"Based on the provided context, point 3 states that where a Certificate of Entitlement has been granted to the Official Participant in respect of imports and supplies covered by Paragraphs (a) and (b) of Clause 1 of Article 2 of this Decision, the Official Participant is required to inform the Bureau.\\n\\nTo elaborate, the Certificate of Entitlement is a document issued by the Authority in response to the request made by the Bureau Expo 2020 Dubai. The Certificate serves as evidence of the entitlement to the refund, and it is typically prepared by the Authority after verifying the refund claim and meeting the agreed-upon requirements.\\n\\nThe Certificate of Entitlement would likely include information such as:\\n\\n- The name of the Official Participant\\n- The amount of the refund\\n- Any other relevant details\\n\\nThe Bureau Expo 2020 Dubai would then use this Certificate to support their request for a refund to the Authority. It's worth noting that the specific requirements for the Certificate of Entitlement are not explicitly stated in the provided context, but it is likely that the Authority would require the Bureau Expo 2020 Dubai to provide documentation or evidence to support the refund claim, and the Certificate would be a formal acknowledgment of the entitlement to the refund.\\n\\nIn this context, the Bureau Expo 2020 Dubai would be required to provide information to the Bureau regarding the Certificate of Entitlement, but the specific requirements for this information are not explicitly stated in the provided context.\")]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
